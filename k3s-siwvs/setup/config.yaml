# k3s configuration optimized for Jetson Nano running ML/AI workloads
# This configuration reduces resource usage while prioritizing ML tasks

# General settings
write-kubeconfig-mode: "0644"
cluster-cidr: "10.42.0.0/16"
service-cidr: "10.43.0.0/16"

# Disable unneeded components to save resources
disable:
  - traefik           # Use custom ingress if needed
  - servicelb         # Use custom load balancer if needed
  - metrics-server    # Can be installed separately if required
  - local-storage     # Use custom storage solution

# Tuned settings for ML workloads on Jetson
kube-controller-manager-arg:
  - "node-monitor-period=60s"
  - "node-monitor-grace-period=150s"
  - "pod-eviction-timeout=300s"
  
# Kubelet tuning for ML workloads
kubelet-arg:
  # Resource reservation and limits
  - "kube-reserved=cpu=100m,memory=100Mi,ephemeral-storage=1Gi"
  - "system-reserved=cpu=100m,memory=200Mi,ephemeral-storage=1Gi"
  - "eviction-hard=memory.available<500Mi,nodefs.available<10%"
  
  # Container GC settings
  - "image-gc-high-threshold=85"
  - "image-gc-low-threshold=80"
  
  # Performance settings
  - "cpu-manager-policy=static"
  - "topology-manager-policy=best-effort"
  
  # CUDA/GPU settings for Jetson
  - "feature-gates=DevicePlugins=true"

# Use etcd as datastore for better reliability
datastore-endpoint: "sqlite:///var/lib/rancher/k3s/server/db/state.db"

# Node labels
node-label:
  - "node-role.kubernetes.io/master=true"
  - "node-role.kubernetes.io/edge=true" 
  - "hardware=jetson-nano"
  - "capability=gpu"
  - "ai-ml-workload=true"

# TLS settings
tls-san:
  - "jetson-nano"
  - "192.168.1.100"  # Replace with your Jetson's actual IP

# Configure service node port range
service-node-port-range: "30000-32767"

# CSI settings for storage
disable-cloud-controller: true
cni: hostport
flannel-backend: host-gw

# Runtime settings
container-runtime-endpoint: "unix:///run/containerd/containerd.sock"
